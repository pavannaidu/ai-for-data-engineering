{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c032d42-7b3b-4038-95c7-bde97aa8eaf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### AI for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "763b0d38-2f97-4688-b78e-05082c2139c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_first_n_primes(n: int) -> List[int]:\n",
    "    \"\"\"\n",
    "    Returns a list of the first n prime numbers.\n",
    "\n",
    "    Parameters:\n",
    "    n (int): Number of prime numbers to return.\n",
    "\n",
    "    Returns:\n",
    "    List[int]: List of the first n prime numbers.\n",
    "    \"\"\"\n",
    "    primes = []\n",
    "    num = 2\n",
    "    while len(primes) < n:\n",
    "        is_prime = True\n",
    "        for i in range(2, int(num ** 0.5) + 1):\n",
    "            if num % i == 0:\n",
    "                is_prime = False\n",
    "                break\n",
    "        if is_prime:\n",
    "            primes.append(num)\n",
    "        num += 1\n",
    "    return primes\n",
    "\n",
    "# Example output\n",
    "print(get_first_n_primes(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24be5648-5e1d-43ca-b520-029cf697051e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def test_get_first_n_primes():\n",
    "    assert get_first_n_primes(0) == [], \"Test case failed for n=0\"\n",
    "    assert get_first_n_primes(1) == [2], \"Test case failed for n=1\"\n",
    "    assert get_first_n_primes(5) == [2, 3, 5, 7, 11], \"Test case failed for n=5\"\n",
    "    assert get_first_n_primes(10) == [2, 3, 5, 7, 11, 13, 17, 19, 23, 29], \"Test case failed for n=10\"\n",
    "\n",
    "test_get_first_n_primes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e29dc51b-5ff2-4d77-90b4-335d1b377fa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the table if it doesn't exist\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS pavan_naidu.demo.people (\n",
    "    id INT,\n",
    "    name STRING,\n",
    "    dob DATE,\n",
    "    gender STRING,\n",
    "    city STRING\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Truncate the table\n",
    "spark.sql(\"TRUNCATE TABLE pavan_naidu.demo.people\")\n",
    "\n",
    "# Sample records\n",
    "data = [\n",
    "    (1, 'Alice', '1990-05-15', 'Female', 'New York'),\n",
    "    (2, 'Bob', '1985-10-23', 'Male', 'Los Angeles'),\n",
    "    (3, 'Charlie', '1978-03-12', 'Male', 'Chicago'),\n",
    "    (4, 'Danielle', '1995-08-02', 'Female', 'San Francisco'),\n",
    "    (5, 'Ethan', '1982-11-30', 'Male', 'Miami')\n",
    "]\n",
    "\n",
    "# Insert sample records into the table\n",
    "df = spark.createDataFrame(data, ['id', 'name', 'dob', 'gender', 'city'])\n",
    "df.write.mode('append').insertInto(\"pavan_naidu.demo.people\")\n",
    "\n",
    "# Display the table\n",
    "display(spark.table(\"pavan_naidu.demo.people\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f5bb56b-8b77-4d52-93df-cd6881b5c207",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS pavan_naidu.demo.millennial\n",
    "LIKE pavan_naidu.demo.people\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f744bc4-2d40-4845-bdd7-312c75f85dd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "MERGE INTO pavan_naidu.demo.millennial AS target\n",
    "USING (\n",
    "    SELECT * FROM pavan_naidu.demo.people\n",
    "    WHERE dob BETWEEN '1981-01-01' AND '1996-12-31'\n",
    ") AS source\n",
    "ON target.id = source.id\n",
    "WHEN MATCHED THEN\n",
    "    UPDATE SET\n",
    "        target.name = source.name,\n",
    "        target.dob = source.dob,\n",
    "        target.gender = source.gender,\n",
    "        target.city = source.city\n",
    "WHEN NOT MATCHED THEN\n",
    "    INSERT *\n",
    "\"\"\")\n",
    "\n",
    "# Display the millennial table\n",
    "display(spark.table(\"pavan_naidu.demo.millennial\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85db1cf8-afb6-4d0d-8099-8730b5224a60",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Millennial Gender Count Analysis"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"c3Bhcmsuc3FsKCIiIgpDUkVBVEUgVEFCTEUgSUYgTk9UIEVYSVNUUyBwYXZhbl9uYWlkdS5kZW1vLm1pbGxlbm5pYWxfc3RhdHMgKAogICAgZ2VuZGVyIFNUUklORywKICAgIGNvdW50IEJJR0lOVAopCiIiIikKCnNwYXJrLnNxbCgiIiIKSU5TRVJUIE9WRVJXUklURSBUQUJMRSBwYXZhbl9uYWlkdS5kZW1vLm1pbGxlbm5pYWxfc3RhdHMKU0VMRUNUIGdlbmRlciwgQ09VTlQoKikgQVMgY291bnQKRlJPTSBwYXZhbl9uYWlkdS5kZW1vLm1pbGxlbm5pYWwKR1JPVVAgQlkgZ2VuZGVyCiIiIikKCmRpc3BsYXkoc3BhcmsudGFibGUoInBhdmFuX25haWR1LmRlbW8ubWlsbGVubmlhbF9zdGF0cyIpKQ==\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksViewe0dfb1b\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksViewe0dfb1b\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksViewe0dfb1b\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksViewe0dfb1b) SELECT `gender`,SUM(`count`) `column_6ae10972109` FROM q GROUP BY `gender`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksViewe0dfb1b\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "bar chart",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "gender",
             "id": "column_6ae10972108"
            },
            "y": [
             {
              "column": "count",
              "id": "column_6ae10972109",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_6ae10972109": {
             "name": "count",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "7c69aee6-efeb-4466-baf2-c8a76c066c12",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 9.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "gender",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "gender",
           "type": "column"
          },
          {
           "alias": "column_6ae10972109",
           "args": [
            {
             "column": "count",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS pavan_naidu.demo.millennial_stats (\n",
    "    gender STRING,\n",
    "    count BIGINT\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "INSERT OVERWRITE TABLE pavan_naidu.demo.millennial_stats\n",
    "SELECT gender, COUNT(*) AS count\n",
    "FROM pavan_naidu.demo.millennial\n",
    "GROUP BY gender\n",
    "\"\"\")\n",
    "\n",
    "display(spark.table(\"pavan_naidu.demo.millennial_stats\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "init",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
